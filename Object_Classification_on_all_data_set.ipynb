{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Object Classification on all data set.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TmLim36iN4m",
        "colab_type": "text"
      },
      "source": [
        "## *Classes*\n",
        "beaver,\n",
        "dolphin, \n",
        "otter, \n",
        "seal,\n",
        "whale, \n",
        "aquarium fish,\n",
        "flatfish, \n",
        "ray,\n",
        "shark,\n",
        "trout, \n",
        "orchids,\n",
        "poppies,\n",
        "roses,\n",
        "sunflowers,\n",
        "tulips, \n",
        "bottles,\n",
        "bowls, \n",
        "cans,\n",
        "cups,\n",
        "plates, \n",
        "apples,\n",
        "mushrooms,\n",
        "oranges,\n",
        "pears,\n",
        "sweet peppers, \n",
        "clock, \n",
        "computer keyboard,\n",
        "lamp,\n",
        "telephone,\n",
        "television, \n",
        "bed,\n",
        "chair, \n",
        "couch,\n",
        "table,\n",
        "wardrobe, \n",
        "bee,\n",
        "beetle,\n",
        "butterfly,\n",
        "caterpillar,\n",
        "cockroach, \n",
        "bear,\n",
        "leopard,\n",
        "lion,\n",
        "tiger, \n",
        "wolf, \n",
        "bridge, \n",
        "castle, \n",
        "house,\n",
        "road, \n",
        "skyscraper, \n",
        "cloud,\n",
        "forest,\n",
        "mountain,\n",
        "plain,\n",
        "sea, \n",
        "camel,\n",
        "cattle,\n",
        "chimpanzee, \n",
        "elephant,\n",
        "kangaroo, \n",
        "fox,\n",
        "porcupine,\n",
        "possum, \n",
        "raccoon, \n",
        "skunk, \n",
        "crab,\n",
        "lobster, \n",
        "snail,\n",
        "spider,\n",
        "worm, \n",
        "baby,\n",
        "boy,\n",
        "girl,\n",
        "man,\n",
        "woman, \n",
        "crocodile,\n",
        "dinosaur, \n",
        "lizard, \n",
        "snake,\n",
        "turtle, \n",
        "hamster,\n",
        "mouse,\n",
        "rabbit,\n",
        "shrew,\n",
        "squirrel, \n",
        "maple,\n",
        "oak,\n",
        "palm,\n",
        "pine,\n",
        "willow, \n",
        "bicycle,\n",
        "bus,\n",
        "motorcycle,\n",
        "pickup truck,\n",
        "train, \n",
        "lawn-mower,\n",
        "rocket,\n",
        "streetcar,\n",
        "tank, \n",
        "tractor,\n",
        "T-shirt/top,\n",
        "Trouser,\n",
        "Pullover,\n",
        "Dress,\n",
        "Coat,\n",
        "Sandal,\n",
        "Shirt,\n",
        "Sneaker,\n",
        "Bag,\n",
        "Ankle boot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "gaK9samciN4p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Datasets\n",
        "from tensorflow.keras.datasets import cifar100 #color images\n",
        "from tensorflow.keras.datasets import cifar10  #color images\n",
        "from tensorflow.keras.datasets import fashion_mnist  # classes describe that fashion mnist was needed instead of mnist (Number) : Grey images of varied size\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import PIL #pillow lib\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "# from sklearn.model_selection import train_test_split # not needed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "v-B8t6zHiN4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Getting data using built-in tensorflow.keras.datasets API\n",
        "(input_train_cifar10, target_train_cifar10), (input_test_cifar10, target_test_cifar10) = cifar10.load_data()\n",
        "(input_train_cifar100, target_train_cifar100), (input_test_cifar100, target_test_cifar100) = cifar100.load_data()\n",
        "(input_train_mnist, target_train_mnist), (input_test_mnist, target_test_mnist) = fashion_mnist.load_data() #fashion mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "id": "GuhnMkzViN5C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "5acfaa40-c823-416a-d512-0b2f624a247e"
      },
      "source": [
        "#TEST SET\n",
        "\n",
        "test_features_set_1 = [] #Storing images combined from all datasets\n",
        "test_labels_set_1 = []   #Storing labels combined from all datasets\n",
        "result = []        \n",
        "start = 100               #Index for data added from other datasets\n",
        "\n",
        "#Add CIFAR-100 data to train_features_set_1 after converting to grayscale and removing RGB layer\n",
        "for i in range(len(input_test_cifar100)):\n",
        "    test_features_set_1.append(np.mean(input_test_cifar100[i],axis=2).reshape((32,32,1))) #reshaped  and\n",
        "    test_labels_set_1.append(target_test_cifar100[i])\n",
        "    \n",
        "#Add CIFAR-10 data to train_features_set_1 after converting to grayscale and removing RGB layer\n",
        "for i in range(len(input_test_cifar10)):\n",
        "    if i == 1: continue\n",
        "    result = np.where(target_test_cifar10 == i)\n",
        "    result = result[0][:500]\n",
        "    for j in result:\n",
        "        test_features_set_1.append(np.mean(input_test_cifar10[j],axis=2).reshape((32,32,1)))\n",
        "        test_labels_set_1.append([start])\n",
        "    start = start + 1\n",
        "\n",
        "#Add MNIST-Fashion after resizing to 32 x 32 and converting to (32,32,1) format\n",
        "for i in range(len(input_test_mnist)):\n",
        "    result = np.where(target_test_mnist == i)\n",
        "    result = result[0][:500]\n",
        "    for j in result:\n",
        "        test_features_set_1.append(cv2.resize(input_test_mnist[j],dsize=(32,32),interpolation=cv2.INTER_AREA).reshape((32,32,1)))\n",
        "        test_labels_set_1.append([start])\n",
        "    start = start + 1\n",
        "\n",
        "#Convert both arrays to numpy\n",
        "test_features_set_1 = np.array(test_features_set_1)\n",
        "test_labels_set_1 = np.array(test_labels_set_1)\n",
        "\n",
        "#Perform shuffling on arrays. Neural Network gives better accuracy when arrays are shuffled rather than when in order\n",
        "state = np.random.get_state()\n",
        "np.random.shuffle(test_features_set_1)\n",
        "np.random.set_state(state)\n",
        "np.random.shuffle(test_labels_set_1)\n",
        "\n",
        "#Observe the classes and samples per class to make sure of the count\n",
        "print(test_features_set_1.shape)\n",
        "un,count = np.unique(test_labels_set_1,return_counts=True)\n",
        "print(un,count)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(19500, 32, 32, 1)\n",
            "[    0     1     2     3     4     5     6     7     8     9    10    11\n",
            "    12    13    14    15    16    17    18    19    20    21    22    23\n",
            "    24    25    26    27    28    29    30    31    32    33    34    35\n",
            "    36    37    38    39    40    41    42    43    44    45    46    47\n",
            "    48    49    50    51    52    53    54    55    56    57    58    59\n",
            "    60    61    62    63    64    65    66    67    68    69    70    71\n",
            "    72    73    74    75    76    77    78    79    80    81    82    83\n",
            "    84    85    86    87    88    89    90    91    92    93    94    95\n",
            "    96    97    98    99   100   101   102   103   104   105   106   107\n",
            "   108 10099 10100 10101 10102 10103 10104 10105 10106 10107 10108] [100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100\n",
            " 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100\n",
            " 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100\n",
            " 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100\n",
            " 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100\n",
            " 100 100 100 100 100 100 100 100 100 100 500 500 500 500 500 500 500 500\n",
            " 500 500 500 500 500 500 500 500 500 500 500]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0FMRwiFnZ1J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "96239ee9-c0b3-4bc5-b86a-7ded4a2914a8"
      },
      "source": [
        "#TRAIN set\n",
        "\n",
        "train_features_set_1 = [] #Storing images combined from all datasets\n",
        "train_labels_set_1 = []   #Storing labels combined from all datasets\n",
        "result = []        \n",
        "start = 100               #Index for data added from other datasets\n",
        "\n",
        "#Add CIFAR-100 data to train_features_set_1 after converting to grayscale and removing RGB layer\n",
        "for i in range(len(input_train_cifar100)):\n",
        "    train_features_set_1.append(np.mean(input_train_cifar100[i],axis=2).reshape((32,32,1))) #reshaped  and grayscaled\n",
        "    train_labels_set_1.append(target_train_cifar100[i])\n",
        "    \n",
        "#Add CIFAR-10 data to train_features_set_1 after converting to grayscale and removing RGB layer\n",
        "for i in range(len(input_train_cifar10)):\n",
        "    if i == 1: continue\n",
        "    result = np.where(target_train_cifar10 == i)\n",
        "    result = result[0][:500]\n",
        "    for j in result:\n",
        "        train_features_set_1.append(np.mean(input_train_cifar10[j],axis=2).reshape((32,32,1)))\n",
        "        train_labels_set_1.append([start])\n",
        "    start = start + 1\n",
        "\n",
        "#Add MNIST-Fashion after resizing to 32 x 32 and converting to (32,32,1) format\n",
        "for i in range(len(input_train_mnist)):\n",
        "    result = np.where(target_train_mnist == i)\n",
        "    result = result[0][:500]\n",
        "    for j in result:\n",
        "        train_features_set_1.append(cv2.resize(input_train_mnist[j],dsize=(32,32),interpolation=cv2.INTER_AREA).reshape((32,32,1)))\n",
        "        train_labels_set_1.append([start])\n",
        "    start = start + 1\n",
        "\n",
        "#Convert both arrays to numpy\n",
        "train_features_set_1 = np.array(train_features_set_1)\n",
        "train_labels_set_1 = np.array(train_labels_set_1)\n",
        "\n",
        "#Perform shuffling on arrays. Neural Network gives better accuracy when arrays are shuffled rather than when in order\n",
        "state = np.random.get_state()\n",
        "np.random.shuffle(train_features_set_1)\n",
        "np.random.set_state(state)\n",
        "np.random.shuffle(train_labels_set_1)\n",
        "\n",
        "#Observe the classes and samples per class to make sure of the count\n",
        "print(train_features_set_1.shape)\n",
        "un,count = np.unique(train_labels_set_1,return_counts=True)\n",
        "print(un,count)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(59500, 32, 32, 1)\n",
            "[    0     1     2     3     4     5     6     7     8     9    10    11\n",
            "    12    13    14    15    16    17    18    19    20    21    22    23\n",
            "    24    25    26    27    28    29    30    31    32    33    34    35\n",
            "    36    37    38    39    40    41    42    43    44    45    46    47\n",
            "    48    49    50    51    52    53    54    55    56    57    58    59\n",
            "    60    61    62    63    64    65    66    67    68    69    70    71\n",
            "    72    73    74    75    76    77    78    79    80    81    82    83\n",
            "    84    85    86    87    88    89    90    91    92    93    94    95\n",
            "    96    97    98    99   100   101   102   103   104   105   106   107\n",
            "   108 50099 50100 50101 50102 50103 50104 50105 50106 50107 50108] [500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500\n",
            " 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500\n",
            " 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500\n",
            " 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500\n",
            " 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500\n",
            " 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500\n",
            " 500 500 500 500 500 500 500 500 500 500 500]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "TiydyqU_iN5L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model\n",
        "#for such a large dataset, shallow networks dont work, so Pretrained models or very deep networks have to be used Or use a large number of epochs say 100\n",
        "# https://paperswithcode.com/sota/image-classification-on-cifar-100\n",
        "\n",
        "# CNN INSPIRED BY VGG LIKE BLOCK, From scratch\n",
        "\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 1)))\n",
        "model.add(keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(keras.layers.MaxPooling2D((2, 2)))\n",
        "model.add(keras.layers.Dropout(0.2))\n",
        "model.add(keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(keras.layers.MaxPooling2D((2, 2)))\n",
        "model.add(keras.layers.Dropout(0.2))\n",
        "model.add(keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(keras.layers.MaxPooling2D((2, 2)))\n",
        "model.add(keras.layers.Dropout(0.2))\n",
        "model.add(keras.layers.Flatten())\n",
        "model.add(keras.layers.Dense(512, activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(keras.layers.Dropout(0.2))\n",
        "model.add(keras.layers.Dense(256, activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(keras.layers.Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(keras.layers.Dense(119, activation='softmax')) #119 classes\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "ep = 50\n",
        "# model.fit(train_features_set_1, train_labels_set_1, epochs=ep)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWZGDR__l7DP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 834
        },
        "outputId": "c405518d-da32-4786-8dfa-d4467ecb5283"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_18 (Conv2D)           (None, 32, 32, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 32, 32, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 8, 8, 128)         147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 512)               1049088   \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dropout_19 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 119)               30583     \n",
            "=================================================================\n",
            "Total params: 1,497,431\n",
            "Trainable params: 1,497,431\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zq9VjmBQmbkW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "04d3653d-799f-434a-84cf-a6fc417ce022"
      },
      "source": [
        "train_feature = train_features_set_1 \n",
        "train_label = train_labels_set_1\n",
        "test_feature = test_features_set_1 \n",
        "test_label = test_labels_set_1\n",
        "\n",
        "history = model.fit(train_feature, # Features\n",
        "                      train_label, # Target\n",
        "                      epochs=100, # Number of epochs\n",
        "                    verbose = 0,\n",
        "                     batch_size = 64, # Number of observations per batch\n",
        "                      validation_data=(test_feature, test_label)) # Data for evaluation\n",
        "# model.fit(train_features_set_1, train_labels_set_1, epochs=100)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-992d1bf59888>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                     \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                     \u001b[0;34m,\u001b[0m \u001b[0;31m# Number of observations per batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                       validation_data=(test_feature, test_label)) # Data for evaluation\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m# model.fit(train_features_set_1, train_labels_set_1, epochs=100)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sdknLlIovqP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get training and test loss histories\n",
        "training_loss = history.history['loss']\n",
        "test_loss = history.history['val_loss']\n",
        "\n",
        "# Create count of the number of epochs\n",
        "epoch_count = range(1, len(training_loss) + 1)\n",
        "\n",
        "# Visualize loss history\n",
        "plt.plot(epoch_count, training_loss, 'r--')\n",
        "plt.plot(epoch_count, test_loss, 'b-')\n",
        "plt.legend(['Training Loss', 'Test Loss'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLdaXl3rrpGs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}